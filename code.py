import streamlit as st
import requests
import httpx
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import time as t

# === ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ ===
client_id = "R7Q2OeVNhj8wZtNNFBwL"
client_secret = "49E810CBKY"

st.set_page_config(page_title="ë‹¨ë…Â·í†µì‹ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°", layout="wide")
st.title("ğŸ“° ë‹¨ë…Â·í†µì‹ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°")
st.caption("ì„¸ê³„ì¼ë³´ ê²½ì°°íŒ€ ë¼ì¸ë³„ ë³´ê³ ë¥¼ ë„ì™€ì¤ë‹ˆë‹¤. (ë§Œë“ ì´: ìœ¤ì¤€í˜¸, ì—…ë°ì´íŠ¸: 250704)")

# === í‚¤ì›Œë“œ ê·¸ë£¹ (ê³µí†µ) ===
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': [
        'ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
        'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
        'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'
    ],
    'ë§ˆí¬ì¤‘ë¶€': [
        'ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€', 'ë°˜ë¶€íŒ¨ë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ê³µê³µë²”ì£„ìˆ˜ì‚¬ëŒ€',
        'ê¸ˆìœµë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ë§ˆì•½ë²”ì£„ìˆ˜ì‚¬ëŒ€', 'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ì¤‘êµ¬', 
        'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'
    ],
    'ë‚¨ë¶€ì§€ê²€ë²•': ['ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•'],
    'ì˜ë“±í¬ê´€ì•…': [
        'ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
        'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'
    ],
    'ê°•ë‚¨ê´‘ì§„': [
        'ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
        'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€', 'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€'
    ]
}

now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=now.date())
    start_time = st.time_input("ì‹œì‘ ì‹œê°", value=dtime(0, 0))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=now.date())
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°", value=dtime(now.hour, now.minute))

selected_groups = st.multiselect("í‚¤ì›Œë“œ ê·¸ë£¹ ì„ íƒ", options=list(keyword_groups.keys()), default=['ì‹œê²½', 'ì¢…í˜œë¶'])
selected_keywords = [kw for g in selected_groups for kw in keyword_groups[g]]

start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

# === ê¸°ëŠ¥ ì„ íƒë¶€ ===
collect_wire = st.checkbox("í†µì‹ ê¸°ì‚¬", value=True)
collect_naver = st.checkbox("ë‹¨ë…ê¸°ì‚¬", value=True)

# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ===
if "wire_articles" not in st.session_state:
    st.session_state.wire_articles = []
for key in ["naver_articles", "naver_status_text", "naver_progress"]:
    if key not in st.session_state:
        st.session_state[key] = [] if key == "naver_articles" else 0 if key == "naver_progress" else ""

# === í†µì‹ ê¸°ì‚¬ í•¨ìˆ˜ë“¤ ===
def highlight_keywords(text, keywords):
    for kw in keywords:
        text = re.sub(f"({re.escape(kw)})", r'<mark style="background-color: #fffb91">\1</mark>', text)
    return text

def get_content(url, selector):
    try:
        with httpx.Client(timeout=5.0) as client:
            res = client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(res.text, "html.parser")
            content = soup.select_one(selector)
            return content.get_text(separator="\n", strip=True) if content else ""
    except:
        return ""

def fetch_articles_concurrently(article_list, selector):
    results = []
    progress_bar = st.progress(0.0, text="ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    total = len(article_list)
    with ThreadPoolExecutor(max_workers=30) as executor:
        futures = {executor.submit(get_content, art['url'], selector): art for art in article_list}
        for i, future in enumerate(as_completed(futures)):
            art = futures[future]
            try:
                content = future.result()
                if any(kw in content for kw in selected_keywords):
                    art['content'] = content
                    results.append(art)
            except:
                continue
            progress_bar.progress((i + 1) / total, text=f"{i+1}/{total} ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
    progress_bar.empty()
    return results

def parse_yonhap():
    collected, page = [], 1
    st.info("ğŸ” [ì—°í•©ë‰´ìŠ¤] ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    while True:
        url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.list01 > li[data-cid]")
        if not items:
            break
        for item in items:
            cid = item.get("data-cid")
            title_tag = item.select_one(".title01")
            time_tag = item.select_one(".txt-time")
            if not (cid and title_tag and time_tag):
                continue
            try:
                dt = datetime.strptime(f"{start_dt.year}-{time_tag.text.strip()}", "%Y-%m-%d %H:%M").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            except:
                continue
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.story-news.article")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "ì—°í•©ë‰´ìŠ¤", "datetime": dt, "title": title_tag.text.strip(),
                    "url": f"https://www.yna.co.kr/view/{cid}"
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.story-news.article")

def parse_newsis():
    collected, page = [], 1
    st.info("ğŸ” [ë‰´ì‹œìŠ¤] ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    while True:
        url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.articleList2 > li")
        if not items:
            break
        for item in items:
            title_tag = item.select_one("p.tit > a")
            time_tag = item.select_one("p.time")
            if not (title_tag and time_tag):
                continue
            title = title_tag.get_text(strip=True)
            href = title_tag.get("href", "")
            match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_tag.text)
            if not match:
                continue
            dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt:
                return fetch_articles_concurrently(collected, "div.viewer")
            if start_dt <= dt <= end_dt:
                collected.append({
                    "source": "ë‰´ì‹œìŠ¤", "datetime": dt, "title": title,
                    "url": "https://www.newsis.com" + href
                })
        page += 1
    return fetch_articles_concurrently(collected, "div.viewer")

# === ë„¤ì´ë²„ ë‹¨ë…ê¸°ì‚¬ í•¨ìˆ˜ë“¤ ===
def naver_parse_pubdate(pubdate_str):
    try:
        return datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def naver_extract_title_and_body(url):
    try:
        if "n.news.naver.com" not in url:
            return None, None
        html = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if html.status_code != 200:
            return None, None
        soup = BeautifulSoup(html.text, "html.parser")
        title_div = soup.find("div", class_="media_end_head_title")
        content_div = soup.find("div", id="newsct_article")
        title = title_div.get_text(strip=True) if title_div else None
        body = content_div.get_text(separator="\n", strip=True) if content_div else None
        return title, body
    except:
        return None, None

def naver_extract_media_name(url):
    try:
        domain = url.split("//")[-1].split("/")[0]
        parts = domain.split(".")
        if len(parts) >= 3:
            composite_key = f"{parts[-3]}.{parts[-2]}"
        else:
            composite_key = parts[0]
        media_mapping = {
            "chosun": "ì¡°ì„ ", "joongang": "ì¤‘ì•™", "donga": "ë™ì•„", "hani": "í•œê²¨ë ˆ",
            "khan": "ê²½í–¥", "hankookilbo": "í•œêµ­", "segye": "ì„¸ê³„", "seoul": "ì„œìš¸",
            "kmib": "êµ­ë¯¼", "munhwa": "ë¬¸í™”", "kbs": "KBS", "sbs": "SBS",
            "imnews": "MBC", "jtbc": "JTBC", "ichannela": "ì±„ë„A", "tvchosun": "TVì¡°ì„ ",
            "mk": "ë§¤ê²½", "sedaily": "ì„œê²½", "hankyung": "í•œê²½", "news1": "ë‰´ìŠ¤1",
            "newsis": "ë‰´ì‹œìŠ¤", "yna": "ì—°í•©", "mt": "ë¨¸íˆ¬", "weekly": "ì£¼ê°„ì¡°ì„ ",
            "biz.chosun": "ì¡°ì„ ë¹„ì¦ˆ", "fnnews": "íŒŒë‰´", "etoday.co": "ì´íˆ¬ë°ì´", "edaily.co": "ì´ë°ì¼ë¦¬", "tf.co": "ë”íŒ©íŠ¸", 
            "yonhapnewstv.co": "ì—°ë‰´TV", "ytn.co": "YTN", "nocutnews.co": "ë…¸ì»·", "biz.heraldcorp": "í—¤ê²½",
            "www.sisajournal": "ì‹œì‚¬ì €ë„", "www.ohmynews": "ì˜¤ë§ˆì´", "dailian.co": "ë°ì¼ë¦¬ì•ˆ", "ilyo.co": "ì¼ìš”ì‹ ë¬¸", "sisain.co": "ì‹œì‚¬IN"
        }
        if composite_key in media_mapping:
            return media_mapping[composite_key]
        for part in reversed(parts):
            if part in media_mapping:
                return media_mapping[part]
        return composite_key.upper()
    except:
        return "[ë§¤ì²´ì¶”ì¶œì‹¤íŒ¨]"

def naver_safe_api_request(url, headers, params, max_retries=3):
    for _ in range(max_retries):
        try:
            res = requests.get(url, headers=headers, params=params, timeout=5)
            if res.status_code == 200:
                return res
            t.sleep(0.5)
        except:
            t.sleep(0.5)
    return res

def naver_fetch_and_filter(item_data):
    item, start_dt, end_dt, selected_keywords, use_keyword_filter = item_data
    link = item.get("link")
    if not link or "n.news.naver.com" not in link:
        return None

    title, body = naver_extract_title_and_body(link)
    if not title or "[ë‹¨ë…]" not in title or not body:
        return None

    pub_dt = naver_parse_pubdate(item.get("pubDate"))
    if not pub_dt or not (start_dt <= pub_dt <= end_dt):
        return None

    matched_keywords = []
    if use_keyword_filter and selected_keywords:
        matched_keywords = [kw for kw in selected_keywords if kw in body]
        if not matched_keywords:
            return None

    highlighted_body = body
    for kw in matched_keywords:
        highlighted_body = highlighted_body.replace(kw, f"<mark>{kw}</mark>")
    highlighted_body = highlighted_body.replace("\n", "<br><br>")
    media = naver_extract_media_name(item.get("originallink", ""))

    return {
        "í‚¤ì›Œë“œ": "[ë‹¨ë…]",
        "ë§¤ì²´": media,
        "ì œëª©": title,
        "ë‚ ì§œ": pub_dt.strftime("%Y-%m-%d %H:%M:%S"),
        "ë³¸ë¬¸": body,
        "í•„í„°ì¼ì¹˜": ", ".join(matched_keywords),
        "ë§í¬": link,
        "í•˜ì´ë¼ì´íŠ¸": highlighted_body,
        "pub_dt": pub_dt
    }

# === ìˆ˜ì§‘ ë²„íŠ¼ ===
if st.button("âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘"):
    if collect_wire:
        st.info("í†µì‹ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        newsis_articles = parse_newsis()
        yonhap_articles = parse_yonhap()
        st.session_state.wire_articles = newsis_articles + yonhap_articles
        st.success(f"âœ… í†µì‹ ê¸°ì‚¬ {len(st.session_state.wire_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

    if collect_naver:
        use_keyword_filter = st.checkbox("ğŸ“ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ë§Œ í•„í„°ë§", value=True, key="naver_filter_run")
        st.info("ë‹¨ë…ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        seen_links = set()
        all_articles = []
        total = 0
        
        progress_bar = st.empty() 
        
        steps = list(range(1, 1001, 100))
        num_steps = len(steps)
        for i, start_index in enumerate(steps, 1):
            progress = i / num_steps
            progress_bar.progress(progress, text=f"ë‹¨ë…ê¸°ì‚¬ {total}ê±´ ìˆ˜ì§‘ ì¤‘")
            params = {
                "query": "[ë‹¨ë…]",
                "sort": "date",
                "display": 100,
                "start": start_index
            }
            res = naver_safe_api_request("https://openapi.naver.com/v1/search/news.json", headers, params)
            if res.status_code != 200:
                st.warning(f"API í˜¸ì¶œ ì‹¤íŒ¨: {res.status_code}")
                break
            items = res.json().get("items", [])
            if not items:
                break

            with ThreadPoolExecutor(max_workers=25) as executor:
                futures = [
                    executor.submit(naver_fetch_and_filter, (item, start_dt, end_dt, selected_keywords, use_keyword_filter))
                    for item in items
                ]
                for future in as_completed(futures):
                    result = future.result()
                    if result and result["ë§í¬"] not in seen_links:
                        seen_links.add(result["ë§í¬"])
                        all_articles.append(result)
                        total += 1
        st.session_state["naver_articles"] = all_articles
        st.success(f"âœ… ë‹¨ë…ê¸°ì‚¬ {len(all_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

# === ê²°ê³¼ ì¶œë ¥ ===
if collect_wire:
    st.header("â—†í†µì‹ ê¸°ì‚¬")
    selected_articles = []
    articles = st.session_state.wire_articles
    if articles:
        for i, art in enumerate(articles):
            expander_key = f"wire_expander_{i}"
            checkbox_key = f"wire_{i}"
        
            # expander ì´ˆê¸°ê°’: ì²´í¬ë°•ìŠ¤ê°€ ì„ íƒëœ ê²½ìš° True, ì•„ë‹ˆë©´ False
            if expander_key not in st.session_state:
                st.session_state[expander_key] = False
        
            # ì²´í¬ë°•ìŠ¤ ìƒíƒœê°€ Trueë¼ë©´ expanderë„ Trueë¡œ!
            if st.session_state.get(checkbox_key, False):
                st.session_state[expander_key] = True

            # ë§¤ ê¸°ì‚¬ë³„ë¡œ ì¼ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            if "content" in art:
                matched_kw = [kw for kw in selected_keywords if kw in art["content"]]
            else:
                matched_kw = []

            with st.expander(art["title"], expanded=st.session_state[expander_key]):
                is_selected = st.checkbox("ì´ ê¸°ì‚¬ ì„ íƒ", key=checkbox_key)
                st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({art['url']})")
                dt_str = art["datetime"].strftime('%Y-%m-%d %H:%M') if "datetime" in art else ""
                st.markdown(f"{art['source']} | {dt_str} | í•„í„°ë§ í‚¤ì›Œë“œ: {', '.join(matched_kw)}")
                if "content" in art:
                    st.markdown(highlight_keywords(art["content"], matched_kw).replace("\n", "<br>"), unsafe_allow_html=True)
                if is_selected:
                    selected_articles.append(art)

        if selected_articles:
            st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ë§Œ)")
            text_block = "ã€ì‚¬íšŒë©´ã€‘\n"
            for row in selected_articles:
                text_block += f"â–³{row['title']}\n-{row['content'].strip()}\n\n"
            st.code(text_block.strip(), language="markdown")
            st.caption("âœ… ë³µì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„ íƒí•œ ê¸°ì‚¬ ë‚´ìš©ì„ ë³µì‚¬í•˜ì„¸ìš”.")
        elif articles:
            st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ ì—†ìŒ)")
            st.info("ì²´í¬ë°•ìŠ¤ë¡œ ê¸°ì‚¬ ì„ íƒ ì‹œ ì´ ì˜ì—­ì— í…ìŠ¤íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

if collect_naver:
    st.header("â—†ë‹¨ë…ê¸°ì‚¬")
    selected_naver_articles = []
    naver_articles = st.session_state["naver_articles"]

    for idx, result in enumerate(naver_articles):
        expander_key = f"naver_expander_{idx}"
        checkbox_key = f"naver_chk_{idx}"
    
        if expander_key not in st.session_state:
            st.session_state[expander_key] = False
    
        if st.session_state.get(checkbox_key, False):
            st.session_state[expander_key] = True
    
        with st.expander(f"{result['ë§¤ì²´']}/{result['ì œëª©']}", expanded=st.session_state[expander_key]):
            is_selected = st.checkbox("ì´ ê¸°ì‚¬ ì„ íƒ", key=checkbox_key)
            st.markdown(f"[ğŸ”— ì›ë¬¸ ë³´ê¸°]({result['ë§í¬']})", unsafe_allow_html=True)
            st.caption(result["ë‚ ì§œ"])
            if result["í•„í„°ì¼ì¹˜"]:
                st.write(f"**ì¼ì¹˜ í‚¤ì›Œë“œ:** {result['í•„í„°ì¼ì¹˜']}")
            st.markdown(f"- {result['í•˜ì´ë¼ì´íŠ¸']}", unsafe_allow_html=True)
            if is_selected:
                selected_naver_articles.append(result)

    if selected_naver_articles:
        st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ë§Œ)")
        text_block = "ã€íƒ€ì§€ã€‘\n"
        for row in selected_naver_articles:
            clean_title = re.sub(r"\[ë‹¨ë…\]|\(ë‹¨ë…\)|ã€ë‹¨ë…ã€‘|â“§ë‹¨ë…|^ë‹¨ë…\s*[:-]?", "", row['ì œëª©']).strip()
            text_block += f"â–³{row['ë§¤ì²´']}/{clean_title}\n-{row['ë³¸ë¬¸']}\n\n"
        st.code(text_block.strip(), language="markdown")
        st.caption("âœ… ë³µì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„ íƒí•œ ê¸°ì‚¬ ë‚´ìš©ì„ ë³µì‚¬í•˜ì„¸ìš”.")
    elif naver_articles:
        st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ (ì„ íƒëœ ê¸°ì‚¬ ì—†ìŒ)")
        st.info("ì²´í¬ë°•ìŠ¤ë¡œ ê¸°ì‚¬ ì„ íƒ ì‹œ ì´ ì˜ì—­ì— í…ìŠ¤íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
